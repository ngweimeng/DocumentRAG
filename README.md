# RAG on NVIDEA's 2023 Annual Report

## Background

Access to Large Language Models (LLMs) such as OpenAI's GPT-4 has been revolutionized through user-friendly APIs, yet the need for self-hosted solutions persists due to strict data privacy and residency requirements. The rise of open-source LLMs offers an alternative to third-party providers, granting more control over model deployment.

Hosting LLMs in-house or on cloud services introduces compute capacity challenges. GPUs, while powerful, incur high costs. This project explores running quantized open-source LLMs on CPUs for document Q&A tasks using Retrieval Augmented Generation (RAG)
